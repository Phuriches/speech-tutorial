{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = 6\n",
    "num_actions = 2\n",
    "\n",
    "terminal_left_reward = 100\n",
    "terminal_right_reward = 40\n",
    "each_step_reward = 0\n",
    "gamma = 0.5 # discount factor\n",
    "misstep_prob = 0 # probability of going in the wrong direction\n",
    "\n",
    "def generate_visualization(\n",
    "    terminal_left_reward,\n",
    "    terminal_right_reward,\n",
    "    each_step_reward,\n",
    "    gamma,\n",
    "    misstep_prob,\n",
    "    ):\n",
    "\n",
    "    reward_steps = np.array([terminal_left_reward] + \n",
    "                            [each_step_reward]*(num_states - 2) + \n",
    "                            [terminal_right_reward]).astype(np.float32)\n",
    "    # find optimal policy\n",
    "    always_left = []\n",
    "    always_right = []\n",
    "\n",
    "    def left_or_right(action, current_state):\n",
    "        action = action\n",
    "        current_state = current_state\n",
    "        if action == 'left':\n",
    "            # all the way to the left\n",
    "            N = len(reward_steps[:current_state])\n",
    "            reward_state = reward_steps[:current_state][::-1]\n",
    "            discount_factors = np.array([gamma**i for i in range(N)])\n",
    "            return_allLeft = np.sum(reward_state * discount_factors)\n",
    "            # after going left, there is a chance of going right one time\n",
    "            # plus one time\n",
    "            if current_state+1 == num_states:\n",
    "                return return_allLeft, 0\n",
    "            else:\n",
    "                discount_factors = np.array([gamma**i for i in range(N+2)])\n",
    "                reward_state = [0]*2 + list(reward_state)\n",
    "                return_rightLeft = np.sum(reward_state * discount_factors)\n",
    "                return return_allLeft, return_rightLeft\n",
    "\n",
    "        else:\n",
    "            # all the way to the right\n",
    "            N = len(reward_steps[current_state-1:])\n",
    "            reward_state = reward_steps[current_state-1:]\n",
    "            discount_factors = np.array([gamma**i for i in range(N)])\n",
    "            return_allRight = np.sum(reward_state * discount_factors)\n",
    "            # after going right, there is a chance of going left one time\n",
    "            # plus one time\n",
    "            if current_state-1 == 0:\n",
    "                return return_allRight, 0\n",
    "            else:\n",
    "                discount_factors = np.array([gamma**i for i in range(N+2)])\n",
    "                reward_state = [0]*2 + list(reward_state)\n",
    "                return_leftRight = np.sum(reward_state * discount_factors)\n",
    "                return return_allRight, return_leftRight\n",
    "    \n",
    "    optimal_policy = []\n",
    "    actions = []\n",
    "    for i in range(1, num_states+1):\n",
    "\n",
    "        if i == 1:\n",
    "            always_left.append(terminal_left_reward)\n",
    "            always_right.append(terminal_left_reward)\n",
    "            actions.append('None')\n",
    "        elif i == num_states:\n",
    "            always_left.append(terminal_right_reward)\n",
    "            always_right.append(terminal_right_reward)\n",
    "            actions.append('None')\n",
    "        else:\n",
    "            return_allLeft, return_rightLeft = left_or_right('left', i)\n",
    "            return_allRight, return_leftRight = left_or_right('right', i)\n",
    "            always_left.append(np.max([return_allLeft, return_leftRight])) # compare lefts\n",
    "            always_right.append(np.max([return_allRight, return_rightLeft])) # compare rights\n",
    "            actions.append('left' if np.argmax([always_left[-1], always_right[-1]]) == 0 else 'right')\n",
    "\n",
    "        optimal_policy.append(np.max([always_left[-1], always_right[-1]]))        \n",
    "    \n",
    "    print(f'----------------- optimal policy -----------------')\n",
    "    print(f'optimal policy: {optimal_policy}')\n",
    "    print(f'actions: {actions}')\n",
    "    print(f'step rewards: {reward_steps}')\n",
    "\n",
    "    print(f'----------------- state-action value function (Q(s, a)) -----------------')\n",
    "    left_list = [float(\"{:0.2f}\".format(a)) for a in always_left]\n",
    "    right_list = [float(\"{:0.2f}\".format(a)) for a in always_right]\n",
    "    state_action = [(i1, i2) for i1, i2 in zip(left_list, right_list)]\n",
    "    print(f'state-action: {state_action}')\n",
    "    print(f'step rewards: {reward_steps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- optimal policy -----------------\n",
      "optimal policy: [100, 50.0, 25.0, 12.5, 20.0, 40]\n",
      "actions: ['None', 'left', 'left', 'left', 'right', 'None']\n",
      "step rewards: [100.   0.   0.   0.   0.  40.]\n",
      "----------------- state-action value function (Q(s, a)) -----------------\n",
      "state-action: [(100.0, 100.0), (50.0, 12.5), (25.0, 6.25), (12.5, 10.0), (6.25, 20.0), (40.0, 40.0)]\n",
      "step rewards: [100.   0.   0.   0.   0.  40.]\n"
     ]
    }
   ],
   "source": [
    "generate_visualization(\n",
    "    terminal_left_reward,\n",
    "    terminal_right_reward,\n",
    "    each_step_reward,\n",
    "    gamma,\n",
    "    misstep_prob,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "pytorch2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
